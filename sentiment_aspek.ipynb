{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54931bd-4974-4def-b2a6-50910261c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Libraries for Text Preprocessing\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# Libraries for Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a4f68-03d1-45b6-9e45-942b7ced5b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv('./data_training/dataset_6.csv')\n",
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91681e8d-2f53-4e5d-a743-2448c5af34d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(1500)\n",
    "tokenizer.fit_on_texts(data_set[\"text\"].values)\n",
    "X=tokenizer.texts_to_sequences(data_set[\"text\"].values)\n",
    "X=pad_sequences(X, maxlen=31, padding='post')\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "data_set.head()\n",
    "print(X)\n",
    "# print(X.shape)\n",
    "# print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc80ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_text = pd.DataFrame({\n",
    "    'original_text':data_set[\"text\"],\n",
    "    'tokenized_text':X[0]\n",
    "})\n",
    "tokenize_text.head(20)\n",
    "# print(prediction_sentiment_result_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c4b43c-5245-467e-b2cf-b40531dd82b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.get_dummies(data_set[[\"sentiment\"]])\n",
    "\n",
    "# switch values for data aspect makanan\n",
    "new_column_order = ['sentiment_positive', 'sentiment_negative']\n",
    "y = y[new_column_order]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train,x_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb914f5-fa2a-445f-bf4b-177315f0ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 32,input_length = 31))\n",
    "model.add(SpatialDropout1D(0.8))\n",
    "model.add(LSTM(64, dropout=0.4, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8853f09d-5df8-4a69-a4fe-79bc0cb3d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hotel yang nyaman, tempat tidur besar, toilet bersih, sarapan enak dan variasi makanan lumayan banyak. \n",
    "# review_test=['pelayanan sangat baik dan ramah pada pengunjung.']\n",
    "review_test = ['kamar pengap tidak ada jendela yang memadai']\n",
    "review_tok=tokenizer.texts_to_sequences(review_test)\n",
    "review_pad=pad_sequences(review_tok,maxlen=31,padding='post')\n",
    "prediction=model.predict(review_pad).flatten()\n",
    "\n",
    "# Convert probabilities to binary class labels\n",
    "threshold = 0.5\n",
    "prediction = tf.nn.softmax(prediction)\n",
    "prediction = tf.where(prediction < threshold, 0, 1)\n",
    "\n",
    "print(\"prediction: \", prediction.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efabb3d-b42b-4eff-93be-bb63534badfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentiment_aspek.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e48d41f2-f92a-429b-93ab-5e286e586ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentimen</th>\n",
       "      <th>content_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kamar yg bersih, fasilitas oke lokasi dekat dg...</td>\n",
       "      <td>positive</td>\n",
       "      <td>kamar yg bersih fasilitas oke lokasi dekat dg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Staffnya ramah dan sigap, untuk menu makanan j...</td>\n",
       "      <td>positive</td>\n",
       "      <td>staffnya ramah dan sigap untuk menu makanan ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Super duper ramah all staff.nya. Hotelnya bers...</td>\n",
       "      <td>positive</td>\n",
       "      <td>super duper ramah all staff nya hotelnya bersi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sarapan sahurnya juga enak banget rasanya. Lok...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sarapan sahurnya juga enak banget rasanya loka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lokasi hotel dekat dengan jalan raya dan ramai...</td>\n",
       "      <td>positive</td>\n",
       "      <td>lokasi hotel dekat dengan jalan raya dan ramai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentimen  \\\n",
       "0  Kamar yg bersih, fasilitas oke lokasi dekat dg...  positive   \n",
       "1  Staffnya ramah dan sigap, untuk menu makanan j...  positive   \n",
       "2  Super duper ramah all staff.nya. Hotelnya bers...  positive   \n",
       "3  Sarapan sahurnya juga enak banget rasanya. Lok...  positive   \n",
       "4  Lokasi hotel dekat dengan jalan raya dan ramai...  positive   \n",
       "\n",
       "                                     content_cleaned  \n",
       "0  kamar yg bersih fasilitas oke lokasi dekat dg ...  \n",
       "1  staffnya ramah dan sigap untuk menu makanan ju...  \n",
       "2  super duper ramah all staff nya hotelnya bersi...  \n",
       "3  sarapan sahurnya juga enak banget rasanya loka...  \n",
       "4  lokasi hotel dekat dengan jalan raya dan ramai...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import neattext.functions as nfx\n",
    "\n",
    "data_validation = pd.read_excel('./data_evaluation/validation_sentimen.xlsx')\n",
    "\n",
    "def cleaning(text):\n",
    "    text = re.sub(r'\\n',' ',text) # Hapus \\n (enter)\n",
    "    # text = nfx.remove_hashtags(text) # Hapus hashtags\n",
    "    text = nfx.remove_numbers(text) # Hapus number\n",
    "    # text = text.strip() # Hapus Whitespace\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) # Hapus karakter selain alfabet dan angka\n",
    "    return text\n",
    "\n",
    "def casefolding(text):\n",
    "    return text.lower()\n",
    "\n",
    "data_validation['content_cleaned'] = data_validation['Text'].apply(cleaning).apply(casefolding)\n",
    "\n",
    "data_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "888dfbc6-3b1e-4e25-b33e-8068bb21b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(data_validation['content_cleaned'])\n",
    "X=tokenizer.texts_to_sequences(data_validation['content_cleaned'])\n",
    "X=pad_sequences(X, maxlen=31, padding='post')\n",
    "\n",
    "# threshold = 0.5\n",
    "# prediction_result = []\n",
    "\n",
    "# predictions_label = model.predict(X)\n",
    "# predictions_label = tf.where(predictions_label < threshold, 0, 1)\n",
    "\n",
    "# for result in predictions_label:\n",
    "#     prediction_result.append(result[0].numpy())\n",
    "\n",
    "# print(prediction_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a94c1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_filenames = ['./aspect_sentiment_models/harga_aspek_dan_sentiment.h5', './aspect_sentiment_models/kamar_aspek_dan_sentiment.h5']  # List of models\n",
    "prediction_sentiments = []\n",
    "threshold = 0.5\n",
    "\n",
    "models = []\n",
    "for filename in model_filenames:\n",
    "    model = load_model(filename)\n",
    "    models.append(model)\n",
    "\n",
    "# Lakukan prediksi pada seluruh data test menggunakan setiap model\n",
    "predictions = np.zeros((X.shape[0], len(models)))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    preds = model.predict(X)\n",
    "    preds_binary = np.where(preds > 0.5, 1, 0)\n",
    "    print(preds_binary)\n",
    "    for j, pred in enumerate(preds_binary):\n",
    "        predictions[j][i] = pred[0]\n",
    "        \n",
    "prediction_aspect_result = predictions.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855594b7-f294-418a-a0d5-ba1d25fdc3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = data_validation['Sentimen'].tolist()\n",
    "sentiment_mapping = {'positive': 1, 'negative': 0}\n",
    "actual_sentiment = [sentiment_mapping[sentiment] for sentiment in data]\n",
    "\n",
    "# Membuat confusion matrix\n",
    "confusion_mat = confusion_matrix(prediction_result, actual_sentiment)\n",
    "\n",
    "confusion_df = pd.DataFrame(confusion_mat, index=['False', 'True'], columns=['False', 'True'])\n",
    "print(confusion_df)\n",
    "\n",
    "# Visualisasi confusion matrix\n",
    "sns.heatmap(confusion_df, annot=True, cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b04f0-35d9-4c7a-b837-846aab501a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy = metrics.accuracy_score(actual_sentiment, prediction_result)\n",
    "F1_score = metrics.f1_score(actual_sentiment, prediction_result)\n",
    "print(\"Accuracy :\", Accuracy)\n",
    "print(\"F1_score :\", F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c78fc6-5906-497d-9347-fcb687580321",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_sentiment_result_convert = pd.DataFrame({\n",
    "    'original_text':data_validation['text'],\n",
    "    'text_cleaned':data_validation['content_cleaned'],\n",
    "    'actual_sentiment': actual_sentiment,\n",
    "    'predicted_sentiment': prediction_result\n",
    "})\n",
    "prediction_sentiment_result_convert.head(20)\n",
    "# print(prediction_sentiment_result_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f8d21-830d-4523-9d09-13f76948bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_sentiment_result_convert.to_csv('result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac1432d-dc9e-450a-a074-24b26d1941c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [ 'kamar pengap tidak ada jendela yang memadai',\n",
    "             'harga terlalu mahal tidak sesuai dengan pelayanan',\n",
    "             'makanan di resto hotel begitu buruk dan tidak enak',\n",
    "             'kamar mandi hotel ini buruk sekali',\n",
    "             'pelayanan buruk, tidak ramah pada pengunjung',\n",
    "             'pelayanan di resto kurang baik',\n",
    "             'kamar yg bersih fasilitas oje lokasi dekat dg penjual makanan serta pelayanan yg ramah',\n",
    "             'respon staff baik dan cepat',\n",
    "             'pelayanan semua ramah dan bagus saya suka',\n",
    "             'fasilitas hotel bagus yang sangat lengkap seperti bintang lima',\n",
    "             'saya suka lokasi hotel strategis dekat banyak rumah makan',\n",
    "             'kamarnya juga bersih dan nyaman',\n",
    "             'makanan dan minuman untuk sarapan pagi bervariasi buat anak-anak suka',\n",
    "             'makanan dan minuman sarapan pagi enak semua',\n",
    "             'makanan di hotel ini lezat',\n",
    "             'kamar mewah dengan balkon pemandangan kota',\n",
    "             'harga terjangkau kualitas bintang lima, saya sangat suka',\n",
    "            ]\n",
    "# test_list = ['tidak suka kamar sempit']\n",
    "text = test_list\n",
    "\n",
    "tokenizer.fit_on_texts(text)\n",
    "test_sequences = tokenizer.texts_to_sequences(text)\n",
    "test_pad = pad_sequences(test_sequences, maxlen=31, padding='post')\n",
    "\n",
    "print(test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78253b3e-109a-4f89-8f8f-7c79163dc0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_filenames = ['./aspect_sentiment_models/harga_aspek_dan_sentiment.h5', './aspect_sentiment_models/kamar_aspek_dan_sentiment.h5', './aspect_sentiment_models/pelayanan_aspek_dan_sentiment.h5']  # List of models\n",
    "prediction_sentiments = []\n",
    "threshold = 0.5\n",
    "\n",
    "models = []\n",
    "for filename in model_filenames:\n",
    "    model = load_model(filename)\n",
    "    models.append(model)\n",
    "\n",
    "predictions = np.zeros((test_pad.shape[0], len(model_filenames)))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    preds = model.predict(test_pad)\n",
    "    preds_binary = np.where(preds > 0.5, 1, 0)\n",
    "    for j, pred in enumerate(preds_binary):\n",
    "        predictions[j][i] = pred[0]\n",
    "\n",
    "prediction_result = predictions.flatten().astype(int)\n",
    "\n",
    "print(prediction_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e507624-0296-46e3-84aa-db7856ce1e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Nilai prediksi dan actual dalam bentuk list of array\n",
    "y_pred = prediction_result\n",
    "y_actual = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Membuat confusion matrix\n",
    "confusion_mat = confusion_matrix(y_pred, y_actual)\n",
    "\n",
    "confusion_df = pd.DataFrame(confusion_mat, index=['False', 'True'], columns=['False', 'True'])\n",
    "print(confusion_df)\n",
    "\n",
    "# Visualisasi confusion matrix\n",
    "sns.heatmap(confusion_df, annot=True, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5fcbda-eb40-4e38-98d0-6a5737338165",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy = metrics.accuracy_score(y_actual, y_pred)\n",
    "F1_score = metrics.f1_score(y_actual, y_pred)\n",
    "print(\"Accuracy :\", Accuracy)\n",
    "print(\"F1_score :\", F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf5a422-3338-401a-a28e-b9f3a5572904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
